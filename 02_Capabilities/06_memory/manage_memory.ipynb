{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df391539",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain_community.chat_models.tongyi import ChatTongyi\n",
    "# model = init_chat_model(\"claude-sonnet-4-5-20250929\")\n",
    "model = ChatTongyi(model=\"qwen-max\", streaming=True)  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6a782e",
   "metadata": {},
   "source": [
    "## 裁剪对话\n",
    "\n",
    "用`trim_message`来裁剪上下文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50ddf91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Bob! How can I assist you further, Bob?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages.utils import (\n",
    "    trim_messages,  \n",
    "    count_tokens_approximately  \n",
    ")\n",
    "from langgraph.graph import StateGraph, START, MessagesState\n",
    "\n",
    "summarization_model = model.bind(max_tokens=128)\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    messages = trim_messages(  \n",
    "        state[\"messages\"],\n",
    "        strategy=\"last\",\n",
    "        token_counter=count_tokens_approximately,\n",
    "        max_tokens=512,  # 128就被裁剪掉第一轮的对话了\n",
    "        start_on=\"human\",\n",
    "        end_on=(\"human\", \"tool\"),\n",
    "    )\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(call_model)\n",
    "builder.add_edge(START, \"call_model\")\n",
    "graph = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "graph.invoke({\"messages\": \"hi, my name is bob\"}, config)\n",
    "graph.invoke({\"messages\": \"write a short poem about cats\"}, config)\n",
    "graph.invoke({\"messages\": \"now do the same but for dogs\"}, config)\n",
    "final_response = graph.invoke({\"messages\": \"what's my name?\"}, config)\n",
    "\n",
    "final_response[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56518d63",
   "metadata": {},
   "source": [
    "# 删除对话\n",
    "\n",
    "用`RemoveMessage`来删除对话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98597e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('human', \"hi! I'm bob\")]\n",
      "[('human', \"hi! I'm bob\"), ('ai', \"Hello Bob! It's nice to meet you. How can I assist you today?\")]\n",
      "[('human', \"hi! I'm bob\"), ('ai', \"Hello Bob! It's nice to meet you. How can I assist you today?\"), ('human', \"what's my name?\")]\n",
      "[('human', \"hi! I'm bob\"), ('ai', \"Hello Bob! It's nice to meet you. How can I assist you today?\"), ('human', \"what's my name?\"), ('ai', \"Your name is Bob! Is there anything else you'd like to know or discuss?\")]\n",
      "[('human', \"what's my name?\"), ('ai', \"Your name is Bob! Is there anything else you'd like to know or discuss?\")]\n"
     ]
    }
   ],
   "source": [
    "from langchain.messages import RemoveMessage  \n",
    "\n",
    "def delete_messages(state):\n",
    "    messages = state[\"messages\"]\n",
    "    if len(messages) > 2:\n",
    "        # remove the earliest two messages\n",
    "        return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]}  \n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_sequence([call_model, delete_messages])\n",
    "builder.add_edge(START, \"call_model\")\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "app = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "for event in app.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n",
    "    config,\n",
    "    stream_mode=\"values\"\n",
    "):\n",
    "    print([(message.type, message.content) for message in event[\"messages\"]])\n",
    "\n",
    "for event in app.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n",
    "    config,\n",
    "    stream_mode=\"values\"\n",
    "):\n",
    "    print([(message.type, message.content) for message in event[\"messages\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa7fb06",
   "metadata": {},
   "source": [
    "# 消息摘要\n",
    "\n",
    "用`SummarizationNode`来获取消息摘要\n",
    "\n",
    "```python\n",
    "summarization_node = SummarizationNode(  \n",
    "    token_counter=count_tokens_approximately,\n",
    "    model=summarization_model,\n",
    "    max_tokens=256,  # 最终给llm的长度\n",
    "    max_tokens_before_summary=256,  # 触发总结的长度\n",
    "    max_summary_tokens=128,  # 最大总结长度\n",
    ")\n",
    "```\n",
    "\n",
    "官方教程给的这个设置就不合理，因为>=256才触发总结，但是触发总结时给llm的上限长度也是256，就意味着>的部分肯定被裁剪了，合理的设置区间应该是max_token > max_tokens_before_summary。\n",
    "\n",
    "上面这个设置就记不住下面的name是bob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4301ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "You mentioned your name earlier as Bob. Is that correct, or did you have a different name in mind?\n",
      "\n",
      "Summary: Sure, here's a short poem about dogs:\n",
      "\n",
      "With tails wagging, joyful and free,\n",
      "Bounding through fields with glee.\n",
      "Eyes full of trust, hearts so true,\n",
      "Loyal friends, steadfast and new.\n",
      "\n",
      "Barking at life with open cheer,\n",
      "In every moment, they're always near.\n",
      "Paws that patter, a warm embrace,\n",
      "Dogs, the light in our daily race.\n",
      "\n",
      "### Summary of the Conversation:\n",
      "- Bob introduced himself.\n",
      "- He requested a short poem about cats, which was provided.\n",
      "- He then asked for a similar poem, but this time about dogs, which was also provided.\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, TypedDict\n",
    "\n",
    "from langchain.messages import AnyMessage\n",
    "from langchain_core.messages.utils import count_tokens_approximately\n",
    "from langgraph.graph import StateGraph, START, MessagesState\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langmem.short_term import SummarizationNode, RunningSummary  \n",
    "\n",
    "summarization_model = model.bind(max_tokens=128)\n",
    "\n",
    "class State(MessagesState):\n",
    "    context: dict[str, RunningSummary]  \n",
    "\n",
    "class LLMInputState(TypedDict):  \n",
    "    summarized_messages: list[AnyMessage]\n",
    "    context: dict[str, RunningSummary]\n",
    "\n",
    "summarization_node = SummarizationNode(  \n",
    "    token_counter=count_tokens_approximately,\n",
    "    model=summarization_model,\n",
    "    max_tokens=256,\n",
    "    max_tokens_before_summary=128,\n",
    "    max_summary_tokens=64,\n",
    ")\n",
    "\n",
    "def call_model(state: LLMInputState):  \n",
    "    response = model.invoke(state[\"summarized_messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(call_model)\n",
    "builder.add_node(\"summarize\", summarization_node)  \n",
    "builder.add_edge(START, \"summarize\")\n",
    "builder.add_edge(\"summarize\", \"call_model\")\n",
    "graph = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "# Invoke the graph\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "graph.invoke({\"messages\": \"hi, my name is bob\"}, config)\n",
    "graph.invoke({\"messages\": \"write a short poem about cats\"}, config)\n",
    "graph.invoke({\"messages\": \"now do the same but for dogs\"}, config)\n",
    "final_response = graph.invoke({\"messages\": \"what's my name?\"}, config)\n",
    "\n",
    "final_response[\"messages\"][-1].pretty_print()\n",
    "print(\"\\nSummary:\", final_response[\"context\"][\"running_summary\"].summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b343aeb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
